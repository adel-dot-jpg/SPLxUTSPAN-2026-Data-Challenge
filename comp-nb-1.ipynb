{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0b0642c5",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2026-02-20T07:38:18.010815Z",
     "iopub.status.busy": "2026-02-20T07:38:18.010494Z",
     "iopub.status.idle": "2026-02-20T07:38:19.228671Z",
     "shell.execute_reply": "2026-02-20T07:38:19.227755Z"
    },
    "papermill": {
     "duration": 1.226269,
     "end_time": "2026-02-20T07:38:19.230054",
     "exception": false,
     "start_time": "2026-02-20T07:38:18.003785",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/kaggle/input/competitions/spl-utspan-data-challenge-2026/tutorial.ipynb\n",
      "/kaggle/input/competitions/spl-utspan-data-challenge-2026/scaler_depth.pkl\n",
      "/kaggle/input/competitions/spl-utspan-data-challenge-2026/scaler_angle.pkl\n",
      "/kaggle/input/competitions/spl-utspan-data-challenge-2026/submission.csv\n",
      "/kaggle/input/competitions/spl-utspan-data-challenge-2026/train.csv\n",
      "/kaggle/input/competitions/spl-utspan-data-challenge-2026/test.csv\n",
      "/kaggle/input/competitions/spl-utspan-data-challenge-2026/scaler_left_right.pkl\n"
     ]
    }
   ],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ebe2ae6",
   "metadata": {
    "papermill": {
     "duration": 0.001458,
     "end_time": "2026-02-20T07:38:19.233337",
     "exception": false,
     "start_time": "2026-02-20T07:38:19.231879",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Methodology and Overview\n",
    "This notebook utilizes an XGBoost regression model. The objective is to predict the scaled angle, depth, and left right deviation of the shot. The full code is available on my public GitHub repository [**here**](https://github.com/adel-dot-jpg/SPLxUTSPAN-2026-Data-Challenge)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c730cd3f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-20T07:38:19.237614Z",
     "iopub.status.busy": "2026-02-20T07:38:19.237268Z",
     "iopub.status.idle": "2026-02-20T07:38:21.566329Z",
     "shell.execute_reply": "2026-02-20T07:38:21.565432Z"
    },
    "papermill": {
     "duration": 2.333617,
     "end_time": "2026-02-20T07:38:21.568354",
     "exception": false,
     "start_time": "2026-02-20T07:38:19.234737",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import ast\n",
    "import xgboost as xgb\n",
    "from sklearn.model_selection import GroupKFold\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.multioutput import MultiOutputRegressor\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore') # Clean up output\n",
    "\n",
    "BASE_PATH = '/kaggle/input/competitions/spl-utspan-data-challenge-2026/'\n",
    "TRAIN_PATH = f'{BASE_PATH}train.csv'\n",
    "TEST_PATH = f'{BASE_PATH}test.csv'\n",
    "SUBMISSION_PATH = f'{BASE_PATH}submission.csv'\n",
    "\n",
    "SCALER_BOUNDS = {\n",
    "    'angle': {'min': 30, 'max': 60},\n",
    "    'depth': {'min': -12, 'max': 30},\n",
    "    'left_right': {'min': -16, 'max': 16}\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "219b7948",
   "metadata": {
    "papermill": {
     "duration": 0.001546,
     "end_time": "2026-02-20T07:38:21.571878",
     "exception": false,
     "start_time": "2026-02-20T07:38:21.570332",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Feature Extraction\n",
    "\n",
    "The raw dataset provides spatial coordinates stored as string representations of lists. To process this, my pipeline uses the ast library (abstract syntax tree) to safely evaluate these strings into NumPy arrays. Because a basketball shot varies in duration and frame count, extracting summary statistics is simpler than feeding raw time series data. For every joint coordinate across the entire duration of the shot, we calculate the mean and standard deviation. This flattens the variable length sequences into a fixed length tabular format suitable for tree based models, capturing both the average position and the physical variability of the shooter's movement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5cacbe16",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-20T07:38:21.576633Z",
     "iopub.status.busy": "2026-02-20T07:38:21.576212Z",
     "iopub.status.idle": "2026-02-20T07:39:25.134813Z",
     "shell.execute_reply": "2026-02-20T07:39:25.133649Z"
    },
    "papermill": {
     "duration": 63.562995,
     "end_time": "2026-02-20T07:39:25.136462",
     "exception": false,
     "start_time": "2026-02-20T07:38:21.573467",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- BEEP, PROCESSING TRAIN DATA ---\n",
      "Loading /kaggle/input/competitions/spl-utspan-data-challenge-2026/train.csv\n",
      "Parsing 345 shots... this might take a moment.\n",
      "Training Data Ready: (345, 414)\n",
      "--- BOOP, PROCESSING TEST DATA ---\n",
      "Loading /kaggle/input/competitions/spl-utspan-data-challenge-2026/test.csv\n",
      "Parsing 113 shots... this might take a moment.\n",
      "Test Data Ready: (113, 414)\n"
     ]
    }
   ],
   "source": [
    "def parse_and_extract_features(path, is_train=True):\n",
    "    print(f\"Loading {path}\")\n",
    "    df = pd.read_csv(path)\n",
    "    \n",
    "    # Identify feature columns (everything that isn't ID or Target)\n",
    "    exclude_cols = ['id', 'shot_id', 'participant_id', 'angle', 'depth', 'left_right', 'Unnamed: 0']\n",
    "    feature_cols = [c for c in df.columns if c not in exclude_cols]\n",
    "    \n",
    "    # Arrays to hold clean data\n",
    "    X_features = []\n",
    "    y_targets = []\n",
    "    groups = []\n",
    "    ids = [] # Keep track of IDs for submission\n",
    "    \n",
    "    print(f\"Parsing {len(df)} shots... this might take a moment.\")\n",
    "    \n",
    "    for idx, row in df.iterrows():\n",
    "        row_stats = []\n",
    "        \n",
    "        for col in feature_cols:\n",
    "            raw_val = row[col]\n",
    "            \n",
    "            try:\n",
    "                if isinstance(raw_val, str):\n",
    "                    series = np.array(ast.literal_eval(raw_val))\n",
    "                else:\n",
    "                    series = np.array([float(raw_val)]) # Handle single numbers if any\n",
    "            except:\n",
    "                series = np.zeros(1) # Fallback\n",
    "            \n",
    "            if len(series) > 0:\n",
    "                row_stats.append(np.mean(series))\n",
    "                row_stats.append(np.std(series))\n",
    "            else:\n",
    "                row_stats.append(0.0)\n",
    "                row_stats.append(0.0)\n",
    "                \n",
    "        X_features.append(row_stats)\n",
    "        ids.append(row.get('id', idx))\n",
    "        \n",
    "        if is_train:\n",
    "            y_targets.append([\n",
    "                row['angle'],\n",
    "                row['depth'],\n",
    "                row['left_right']\n",
    "            ])\n",
    "            # Append participant ID for groupkfold\n",
    "            groups.append(row['participant_id'])\n",
    "            \n",
    "    # Convert to dataframes\n",
    "    X_df = pd.DataFrame(X_features, columns=[f\"{c}_{stat}\" for c in feature_cols for stat in ['mean', 'std']])\n",
    "    \n",
    "    if is_train:\n",
    "        return X_df, pd.DataFrame(y_targets, columns=['angle', 'depth', 'left_right']), np.array(groups)\n",
    "    else:\n",
    "        return X_df, ids\n",
    "\n",
    "# Execute loading\n",
    "print(\"--- BEEP, PROCESSING TRAIN DATA ---\")\n",
    "X, y, participants = parse_and_extract_features(TRAIN_PATH, is_train=True)\n",
    "print(f\"Training Data Ready: {X.shape}\")\n",
    "\n",
    "print(\"--- BOOP, PROCESSING TEST DATA ---\")\n",
    "X_test, test_ids = parse_and_extract_features(TEST_PATH, is_train=False)\n",
    "print(f\"Test Data Ready: {X_test.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "170c6b58",
   "metadata": {
    "papermill": {
     "duration": 0.001748,
     "end_time": "2026-02-20T07:39:25.140132",
     "exception": false,
     "start_time": "2026-02-20T07:39:25.138384",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Model Strategy and Validation\n",
    "\n",
    "For the predictive model, I utilize XGBoost regressor wrapped in a MultiOutputRegressor to handle the three continuous target variables simultaneously. Given the small sample size of 458 shots across only 5 participants, if the model sees the same participant in both the training and validation sets, there is the risk of the model memorizing player body mechanics instead of shot physics. To prevent this, Group K Fold cross validation, grouping by participant ID is employed to validate output consistency. This checks if the model is learning generalized shooting mechanics and evaluates its performance on an entirely unseen participant during each fold."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0b25e5bd",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-20T07:39:25.144944Z",
     "iopub.status.busy": "2026-02-20T07:39:25.144704Z",
     "iopub.status.idle": "2026-02-20T07:39:40.071914Z",
     "shell.execute_reply": "2026-02-20T07:39:40.069739Z"
    },
    "papermill": {
     "duration": 14.931703,
     "end_time": "2026-02-20T07:39:40.073524",
     "exception": false,
     "start_time": "2026-02-20T07:39:25.141821",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- HOLDUP, STARTING CROSS-VALIDATION ---\n",
      "Fold:  1 Player:  [5] Raw MSE =  48.71630859375\n",
      "Fold:  2 Player:  [1] Raw MSE =  28.639997482299805\n",
      "Fold:  3 Player:  [3] Raw MSE =  8.465067863464355\n",
      "Fold:  4 Player:  [4] Raw MSE =  52.89833450317383\n",
      "Fold:  5 Player:  [2] Raw MSE =  32.639408111572266\n",
      "Average Raw MSE:  34.27182331085205\n",
      "Retraining...\n"
     ]
    }
   ],
   "source": [
    "# XGBoost setup\n",
    "model = MultiOutputRegressor(xgb.XGBRegressor(\n",
    "    n_estimators=100,\n",
    "    learning_rate=0.05,\n",
    "    max_depth=3, # Keep shallow to prevent overfitting\n",
    "    subsample=0.8,\n",
    "    colsample_bytree=0.8,\n",
    "    random_state=67\n",
    "))\n",
    "\n",
    "# Validation logic\n",
    "gkf = GroupKFold(n_splits=5)\n",
    "scores = []\n",
    "\n",
    "print(\"--- HOLDUP, STARTING CROSS-VALIDATION ---\")\n",
    "for fold, (train_idx, val_idx) in enumerate(gkf.split(X, y, groups=participants)):\n",
    "    X_train_fold, X_val_fold = X.iloc[train_idx], X.iloc[val_idx]\n",
    "    y_train_fold, y_val_fold = y.iloc[train_idx], y.iloc[val_idx]\n",
    "    \n",
    "    model.fit(X_train_fold, y_train_fold)\n",
    "    preds = model.predict(X_val_fold)\n",
    "    \n",
    "    # Scale the predictions to calculate the true competition metric\n",
    "    mse = mean_squared_error(y_val_fold, preds)\n",
    "    scores.append(mse)\n",
    "    \n",
    "    # Helper to see which player we tested on\n",
    "    val_p = np.unique(participants[val_idx])\n",
    "    print(\"Fold: \", fold+1, \"Player: \", val_p, \"Raw MSE = \", mse)\n",
    "\n",
    "print(\"Average Raw MSE: \", np.mean(scores))\n",
    "\n",
    "# Final training on all data\n",
    "print(\"Retraining...\")\n",
    "model.fit(X, y)\n",
    "final_preds = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3633c207",
   "metadata": {
    "papermill": {
     "duration": 0.001809,
     "end_time": "2026-02-20T07:39:40.077591",
     "exception": false,
     "start_time": "2026-02-20T07:39:40.075782",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Final Prediction and Scaling\n",
    "\n",
    "After validating the model architecture, it is retrained on the entire training dataset to maximize the amount of data it learns from before inferring on the test set. Finally, the raw predictions must be scaled to adhere to the competition requirements. A MinMax scaling formula is applied using the provided bounds for angle, depth, and left right deviation, clipping the final results between 0 and 1 to generate the final submission file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2f72588e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-20T07:39:40.082905Z",
     "iopub.status.busy": "2026-02-20T07:39:40.082624Z",
     "iopub.status.idle": "2026-02-20T07:39:40.106172Z",
     "shell.execute_reply": "2026-02-20T07:39:40.105450Z"
    },
    "papermill": {
     "duration": 0.028141,
     "end_time": "2026-02-20T07:39:40.107522",
     "exception": false,
     "start_time": "2026-02-20T07:39:40.079381",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "submission.csv created successfully!\n",
      "                                     id  scaled_angle  scaled_depth  \\\n",
      "0  d5cc9ade-6bfd-42d2-8404-99d7506e535c      0.483936      0.591083   \n",
      "1  6fb475ff-1732-42bc-8385-9f80956199fe      0.484560      0.523309   \n",
      "2  39f95c12-deab-4d77-8a9c-feecda4d5a66      0.521334      0.538571   \n",
      "3  5ec65bf7-4892-4076-a572-e01b4b8ff038      0.476318      0.522425   \n",
      "4  52ffbd2a-969c-4e52-af66-c4b4be3c3cbb      0.489406      0.595713   \n",
      "\n",
      "   scaled_left_right  \n",
      "0           0.417449  \n",
      "1           0.511225  \n",
      "2           0.492368  \n",
      "3           0.530073  \n",
      "4           0.376594  \n"
     ]
    }
   ],
   "source": [
    "def scale_output(values, col_name):\n",
    "    \"\"\"Applies the competition MinMax scaling formula.\"\"\"\n",
    "    mini = SCALER_BOUNDS[col_name]['min']\n",
    "    maxi = SCALER_BOUNDS[col_name]['max']\n",
    "    \n",
    "    # Formula: (x - min) / (max - min)\n",
    "    scaled = (values - mini) / (maxi - mini)\n",
    "    \n",
    "    # Clip to ensure we don't go outside [0, 1]\n",
    "    return np.clip(scaled, 0, 1)\n",
    "\n",
    "# Create submission DataFrame\n",
    "submission = pd.DataFrame()\n",
    "submission['id'] = test_ids\n",
    "submission['scaled_angle'] = scale_output(final_preds[:, 0], 'angle')\n",
    "submission['scaled_depth'] = scale_output(final_preds[:, 1], 'depth')\n",
    "submission['scaled_left_right'] = scale_output(final_preds[:, 2], 'left_right')\n",
    "\n",
    "# Save\n",
    "submission.to_csv('submission.csv', index=False)\n",
    "print(\"submission.csv created successfully!\")\n",
    "print(submission.head())"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "databundleVersionId": 15218621,
     "sourceId": 126310,
     "sourceType": "competition"
    }
   ],
   "dockerImageVersionId": 31259,
   "isGpuEnabled": false,
   "isInternetEnabled": false,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 85.887533,
   "end_time": "2026-02-20T07:39:40.829330",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2026-02-20T07:38:14.941797",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
